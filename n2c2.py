# -*- coding: utf-8 -*-
"""Untitled74 (3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19vW7_g6rJlzVcIeW-Wdw32pdmvYvzVxZ
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from google.colab import drive
drive.mount('/content/gdrive')

!unzip /content/gdrive/MyDrive/n2c2/trainingdata_v3.zip -d /content/gdrive/MyDrive/n2c2

import pickle
import nltk

nltk.download('punkt')
nltk.download('stopwords')

import os
import numpy as np
import string

import pandas as pd
from nltk.corpus import stopwords

#global variable
Entity_Count = 0;

def Parse_Annotation(filepath = ""):
    Annotation = open(filepath, "r");
    Annotation = Annotation.readlines();

    Ann_List = [];
    for Ann in Annotation:
        Ann = Ann.replace("/", " ");
        Ann = Ann.replace(".", "");
        if Ann[0] == "T":
            Ann = nltk.tokenize.word_tokenize(Ann);
            #Edit tokenized list with
            if len(Ann) > 5 and Ann.count(";")>0:
                Ann = [int(Ann[2]), Ann[1]] + Ann[7:];

                if len(Ann) > 3:
                    BAnn = tuple(Ann[:3]);
                    Ann_List.append(BAnn);
                    Term_index = Ann[0] + 1;
                    for IAnn in Ann[3:]:
                        tag = "I" + Ann[1];
                        IAnn = (Term_index, tag, IAnn);
                        Term_index = Term_index + 1;
                        Ann_List.append(IAnn);
                else:
                    Ann = tuple(Ann);
                    Ann_List.append(Ann);

            else:
                Ann = [int(Ann[2]), Ann[1]] + Ann[4:];

                if len(Ann) > 3:
                    BAnn = tuple(Ann[:3]);
                    Ann_List.append(BAnn);
                    Term_index = Ann[0] + 1;
                    for IAnn in Ann[3:]:
                        tag = "I" + Ann[1];
                        IAnn = (Term_index, tag, IAnn);
                        Term_index = Term_index + 1;
                        Ann_List.append(IAnn);
                else:
                    Ann = tuple(Ann);
                    Ann_List.append(Ann);

    Term_index = []
    Term_dict = {};
    [Term_index.append(x[0]) for x in Ann_List]
    Term_index.sort();
    for Terms in Term_index:
        Term_dict[Terms] = None;
    for tuples in Ann_List:
        Term_dict[tuples[0]] = tuples[1:];

    Ann_List = list(Term_dict.values());
    return Ann_List;

def Tokenize_EHR(file_path = ""):

    EHR_txt = open(file_path, 'r').read();
    EHR_txt = EHR_txt.replace("/", " ");

    EHR_Sections = EHR_txt.split("\n\n")
    EHR_Sents = [];
    for sections in EHR_Sections:
        EHR_Sents.extend(nltk.sent_tokenize(sections));

    EHR_Tokenized = [];
    for sents in EHR_Sents:
        EHR_Tokenized.append(nltk.word_tokenize(sents));

    return EHR_Tokenized;

def Generate_cmed_data(EHR_data_dir = "", EHR_fileID = "", write_file = False):
    Tags = {"Disposition":("B"), "IDisposition":("I"), "NoDisposition":("B"),
           "INoDisposition":("I"), "Undetermined":("B"), "IUndetermined":("I"), "Outside":{"O"}};
    EHR_Ann_filepath = EHR_data_dir + EHR_fileID + ".ann";
    EHR_doc_filepath = EHR_data_dir + EHR_fileID + ".txt";
    EHR_Tagged_filepath = EHR_data_dir + EHR_fileID + "_Tagged.txt";

    EHR_Tokens = Tokenize_EHR(EHR_doc_filepath);
    Parsed_Annotation = Parse_Annotation(EHR_Ann_filepath);
    Parsed_Annotation.append((" ", None));

    Tagged_EHR_List = [];

    if(write_file) :Tagged_EHR_File = open(EHR_Tagged_filepath, "w");
    Annotation_index = 0;

    stop_words = set(stopwords.words('english'))
    removal_list = list(stop_words) + list(string.punctuation) + ['-'];


    for sents in EHR_Tokens:
        for words in sents:
           #Make specific changes relating to dataset here
           if(words in removal_list): continue;
           elif(words[0] == "_"): continue;

           if words == Parsed_Annotation[Annotation_index][1]:
               tag = Tags[Parsed_Annotation[Annotation_index][0]];
               Tagged_EHR_List.append((words, tag))
               if(write_file): Tagged_EHR_File.write(words + " " + tag + "\n");
               Annotation_index = Annotation_index + 1;
           else:
               Tagged_EHR_List.append((words, 'O'));
               if(write_file):Tagged_EHR_File.write(words + " " + '0' + "\n");

        Tagged_EHR_List.append(("", ""));
        if(write_file):Tagged_EHR_File.write("\n");

    if(write_file):Tagged_EHR_File.close();

    return np.array(Tagged_EHR_List).transpose();

def main():

    Working_dir = "/content/gdrive/MyDrive/n2c2/trainingdata_v3/";
    Training_data_dir = Working_dir + "train/";
    Eval_data_dir = Working_dir + "dev/";

    Training_files_list = os.listdir(Training_data_dir);
    Eval_files_list = os.listdir(Eval_data_dir);

    #Test case
    #Generate_cmed_data(EHR_data_dir=Training_data_dir, EHR_fileID="101-02", write_file=True)

    #Build training data for Biobert model
    cmed_data_dict = {"Tokens":[], "Tags":[]};
    Data_files_dict = {};
    for files in Training_files_list:
       Data_files_dict[files[:6]] = None;
    for IDs in Data_files_dict:
        if IDs=='.DS_St':
          continue
        [tokens, tags] = Generate_cmed_data(EHR_data_dir=Training_data_dir, EHR_fileID=IDs, write_file = False);
        cmed_data_dict["Tokens"].extend(tokens);
        cmed_data_dict["Tags"].extend(tags);
        cmed_data = pd.DataFrame(cmed_data_dict);
        train_data_filename = Working_dir + "cmed_train_data.tsv";
        cmed_data.to_csv(train_data_filename, sep=" ",header=False, index = False );

    global Entity_Count;
    print(Entity_Count);

    #Build eval data for Biobert model
    cmed_data_dict = {"Tokens": [], "Tags": []};
    Data_files_dict = {};
    for files in Eval_files_list:
       Data_files_dict[files[:6]] = None;
    for IDs in Data_files_dict:
        if IDs=='.DS_St':
          continue
        [tokens, tags] = Generate_cmed_data(EHR_data_dir=Eval_data_dir, EHR_fileID=IDs, write_file = False);
        cmed_data_dict["Tokens"].extend(tokens);
        cmed_data_dict["Tags"].extend(tags);
        cmed_data = pd.DataFrame(cmed_data_dict);
        train_data_filename = Working_dir + "cmed_eval_data.tsv";
        cmed_data.to_csv(train_data_filename, sep=" ",header=False, index = False );


    return 0;


if __name__ == '__main__':
    main()

import pandas as pd
train_data_path = '/content/gdrive/MyDrive/n2c2/trainingdata_v3/cmed_train_data (2).tsv'

train_df = pd.read_csv(train_data_path,quoting = 3, skip_blank_lines = False)


train_df2=pd.DataFrame()
train_df2[['words','labels']]=train_df['Record O'].str.split(" ", expand=True)
train_df2.head(100)
train_df2.loc[train_df2['labels']=='0','labels']='O'
train_df2.loc[train_df2['labels']=='B','labels']='B-Entity'
train_df2.loc[train_df2['labels']=='I','labels']='I-Entity'
train_df2.head(100)

train_df2['sentence_id'] = (train_df2.words == '').cumsum()
train_df2.head(1000)

train_df2=train_df2.drop(train_df2[(train_df2.words == '') | (train_df2.labels == '')].index)
train_df2

custom_labels = list(train_df2['labels'].unique())
print(custom_labels)

eval_data_path = '/content/gdrive/MyDrive/n2c2/trainingdata_v3/cmed_eval_data (2).tsv'
eval_df = pd.read_csv(eval_data_path, quoting=3, skip_blank_lines=False)

eval_df2=pd.DataFrame()
eval_df2[['words','labels']]=eval_df['Record O'].str.split(" ", expand=True)
eval_df2.head(100)

eval_df2.loc[eval_df2['labels']=='0','labels']='O'
eval_df2.loc[eval_df2['labels']=='B','labels']='B-Entity'
eval_df2.loc[eval_df2['labels']=='I','labels']='I-Entity'

eval_df2['sentence_id'] = (eval_df2.words == '').cumsum()
eval_df2.head(1000)

eval_df2=eval_df2.drop(eval_df2[ ((eval_df2.words == '') | (eval_df2.labels == ''))].index)
eval_df2

!pip install -q transformers
!pip install -q simpletransformers

train_args = {
    'reprocess_input_data': True,
    'overwrite_output_dir': True,
    'sliding_window': True,
    'max_seq_length': 32,
    'num_train_epochs': 1,
    'train_batch_size': 32,
    'fp16': True,
    'output_dir': '/outputs/',
    'best_model_dir': '/outputs/best_model/',
    'evaluate_during_training': True,
}

from simpletransformers.ner import NERModel
from transformers import AutoTokenizer
import pandas as pd
import logging
logging.basicConfig(level=logging.DEBUG)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

# We use the bio BERT pre-trained model.
model = NERModel('bert', 'dmis-lab/biobert-v1.1',labels=custom_labels, args=train_args, use_cuda=False)

model.train_model(train_data=train_df2, eval_data= eval_df2)